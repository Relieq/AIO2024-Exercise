{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-and-cat-detection\")\n",
        "print(\"Path to dataset files:\", data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfuqNLfAofSy",
        "outputId": "74e90df9-c2e9-432c-b0e6-0e910e97545a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/andrewmvd/dog-and-cat-detection?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.03G/1.03G [00:14<00:00, 75.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/andrewmvd/dog-and-cat-detection/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YBwl4YhhobeD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
        "        self.annotations_dir = annotations_dir\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = self.filter_images_with_multiple_objects()\n",
        "\n",
        "    def filter_images_with_multiple_objects(self):\n",
        "        valid_image_files = []\n",
        "        for f in os.listdir(self.image_dir):\n",
        "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
        "                img_name = f\n",
        "                annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "                # Keep images that have single object\n",
        "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
        "                    valid_image_files.append(img_name)\n",
        "        return valid_image_files\n",
        "\n",
        "    def count_objects_in_annotation(self, annotation_path):\n",
        "        try:\n",
        "            tree = ET.parse(annotation_path)\n",
        "            root = tree.getroot()\n",
        "            count = 0\n",
        "            for obj in root.findall(\"object\"):\n",
        "                count += 1\n",
        "            return count\n",
        "        except FileNotFoundError:\n",
        "            return 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Image path\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Annotation path\n",
        "        annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
        "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
        "\n",
        "        # Parse annotation\n",
        "        label, bbox = self.parse_annotation(annotation_path)  # Get both label and bbox\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label, bbox\n",
        "\n",
        "    def parse_annotation(self, annotation_path):\n",
        "        tree = ET.parse(annotation_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Get image size for normalization\n",
        "        image_width = int(root.find(\"size/width\").text)\n",
        "        image_height = int(root.find(\"size/height\").text)\n",
        "\n",
        "        label = None\n",
        "        bbox = None\n",
        "        for obj in root.findall(\"object\"):\n",
        "            name = obj.find(\"name\").text\n",
        "            if label is None:  # Take the first label\n",
        "                label = name\n",
        "            # Get bounding box coordinates\n",
        "            xmin = int(obj.find(\"bndbox/xmin\").text)\n",
        "            ymin = int(obj.find(\"bndbox/ymin\").text)\n",
        "            xmax = int(obj.find(\"bndbox/xmax\").text)\n",
        "            ymax = int(obj.find(\"bndbox/ymax\").text)\n",
        "\n",
        "            # Normalize bbox coordinates to [0, 1]\n",
        "            bbox = [\n",
        "                xmin / image_width,\n",
        "                ymin / image_height,\n",
        "                xmax / image_width,\n",
        "                ymax / image_height,\n",
        "            ]\n",
        "\n",
        "        # Convert label to numerical representation (0 for cat, 1 for dog)\n",
        "        label_num = 0 if label == \"cat\" else 1 if label == \"dog\" else -1\n",
        "        return label_num, torch.tensor(bbox, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "BegcJnXxojuj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_dir = os.path.join(data_dir, 'annotations')\n",
        "image_dir = os.path.join(data_dir, 'images')\n",
        "\n",
        "# Get list of image files and create a dummy dataframe to split the data\n",
        "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
        "df = pd.DataFrame({'image_name': image_files})\n",
        "\n",
        "# Split data\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "arfUX220omOA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Datasets\n",
        "train_dataset = ImageDataset(annotations_dir, image_dir, transform=transform)\n",
        "val_dataset = ImageDataset(annotations_dir, image_dir, transform=transform)\n",
        "\n",
        "# Filter datasets based on train_df and val_df\n",
        "train_dataset.image_files = [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
        "val_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "o13rkidXoqNB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoHeadedModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(TwoHeadedModel, self).__init__()\n",
        "        self.base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        self.num_ftrs = self.base_model.fc.in_features\n",
        "\n",
        "        # Remove the original fully connected layer\n",
        "        self.base_model.fc = nn.Identity()\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(self.num_ftrs, num_classes)\n",
        "\n",
        "        # Bounding box regression head\n",
        "        self.regressor = nn.Linear(self.num_ftrs, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        class_logits = self.classifier(x)\n",
        "        bbox_coords = torch.sigmoid(self.regressor(x))  # Normalize bbox coordinates to [0, 1]\n",
        "        return class_logits, bbox_coords\n",
        "\n",
        "model = TwoHeadedModel()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_bbox = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXTWMEevorUP",
        "outputId": "428b3925-f552-499e-eb77-d25610b9f606"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 61.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, targets, bboxes) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        bboxes = bboxes.to(device)\n",
        "\n",
        "        scores, pred_bboxes = model(data)\n",
        "        loss_class = criterion_class(scores, targets)\n",
        "        loss_bbox = criterion_bbox(pred_bboxes, bboxes)\n",
        "        loss = loss_class + loss_bbox  # Combine losses\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        total_loss_bbox = 0\n",
        "        total_samples = 0\n",
        "        for data, targets, bboxes in val_loader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            bboxes = bboxes.to(device)\n",
        "            scores, pred_bboxes = model(data)\n",
        "            _, predictions = scores.max(1)\n",
        "            correct += (predictions == targets).sum()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            total_loss_bbox += criterion_bbox(pred_bboxes, bboxes).item() * data.size(0)\n",
        "            total_samples += data.size(0)\n",
        "\n",
        "        avg_loss_bbox = total_loss_bbox / total_samples\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {float(correct)/float(total)*100:.2f}%, Avg. Bbox Loss: {avg_loss_bbox:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDMG_bnsow1j",
        "outputId": "c094d464-9185-4655-8167-8074d37562fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Validation Accuracy: 92.40%, Avg. Bbox Loss: 0.0164\n",
            "Epoch 2/10, Validation Accuracy: 93.22%, Avg. Bbox Loss: 0.0108\n",
            "Epoch 3/10, Validation Accuracy: 88.60%, Avg. Bbox Loss: 0.0212\n",
            "Epoch 4/10, Validation Accuracy: 92.54%, Avg. Bbox Loss: 0.0115\n",
            "Epoch 5/10, Validation Accuracy: 95.25%, Avg. Bbox Loss: 0.0082\n",
            "Epoch 6/10, Validation Accuracy: 90.91%, Avg. Bbox Loss: 0.0113\n",
            "Epoch 7/10, Validation Accuracy: 94.71%, Avg. Bbox Loss: 0.0098\n",
            "Epoch 8/10, Validation Accuracy: 90.50%, Avg. Bbox Loss: 0.0125\n",
            "Epoch 9/10, Validation Accuracy: 93.49%, Avg. Bbox Loss: 0.0092\n",
            "Epoch 10/10, Validation Accuracy: 94.57%, Avg. Bbox Loss: 0.0085\n"
          ]
        }
      ]
    }
  ]
}