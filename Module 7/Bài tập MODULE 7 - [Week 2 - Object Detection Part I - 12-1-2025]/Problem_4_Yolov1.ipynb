{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bisaTiJstwSb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches\n",
        "from collections import Counter\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from termcolor import colored\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomVOCDataset(torchvision.datasets.VOCDetection):\n",
        "    def init_config_yolo(self, class_mapping, S=7, B=2, C=20, custom_transforms=None):\n",
        "        self.S = S  # Grid size S x S\n",
        "        self.B = B  # Number of bounding boxes\n",
        "        self.C = C  # Number of classes\n",
        "        self.class_mapping = class_mapping  # Mapping of class names to class indices\n",
        "        self.custom_transforms = custom_transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, target = super(CustomVOCDataset, self).__getitem__(index)\n",
        "        img_width, img_height = image.size\n",
        "\n",
        "        # Convert target annotations to YOLO format bounding boxes\n",
        "        boxes = convert_to_yolo_format(target, img_width, img_height, self.class_mapping)\n",
        "\n",
        "        # Transform\n",
        "        if self.custom_transforms:\n",
        "            sample = {\n",
        "                'image': np.array(image),\n",
        "                'bboxes': boxes[:, 1:],  # Exclude class_id\n",
        "                'labels': boxes[:, 0]    # Class_id\n",
        "            }\n",
        "            sample = self.custom_transforms(**sample)\n",
        "            image = sample['image']\n",
        "            boxes = sample['bboxes']\n",
        "            labels = sample['labels']\n",
        "\n",
        "        # Create an empty label matrix for YOLO ground truth\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.float32)\n",
        "        image = torch.as_tensor(image, dtype=torch.float32)\n",
        "\n",
        "        # Iterate through each bounding box in YOLO format\n",
        "        for box, class_label in zip(boxes, labels):\n",
        "            x, y, width, height = box.tolist()\n",
        "            class_label = int(class_label)\n",
        "\n",
        "            # Calculate the grid cell (i, j) that this box belongs to\n",
        "            i, j = int(self.S * y), int(self.S * x)\n",
        "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "\n",
        "            # Calculate the width and height of the box relative to the grid cell\n",
        "            width_cell, height_cell = width * self.S, height * self.S\n",
        "\n",
        "            # If no object has been found in this specific cell (i, j) before:\n",
        "            if label_matrix[i, j, 20] == 0:\n",
        "                # Mark that an object exists in this cell\n",
        "                label_matrix[i, j, 20] = 1\n",
        "\n",
        "                # Store the box coordinates as an offset from the cell boundaries\n",
        "                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
        "\n",
        "                # Set the box coordinates in the label matrix\n",
        "                label_matrix[i, j, 21:25] = box_coordinates\n",
        "\n",
        "                # Set the one-hot encoding for the class label\n",
        "                label_matrix[i, j, class_label] = 1\n",
        "\n",
        "        return image, label_matrix"
      ],
      "metadata": {
        "id": "JqNCbe_TvCA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_yolo_format(target, img_width, img_height, class_mapping):\n",
        "    annotations = target['annotation']['object']\n",
        "    real_width = int(target['annotation']['size']['width'])\n",
        "    real_height = int(target['annotation']['size']['height'])\n",
        "\n",
        "    if not isinstance(annotations, list):\n",
        "        annotations = [annotations]\n",
        "\n",
        "    boxes = []\n",
        "    for anno in annotations:\n",
        "        xmin = int(anno['bndbox']['xmin']) / real_width\n",
        "        xmax = int(anno['bndbox']['xmax']) / real_width\n",
        "        ymin = int(anno['bndbox']['ymin']) / real_height\n",
        "        ymax = int(anno['bndbox']['ymax']) / real_height\n",
        "\n",
        "        x_center = (xmin + xmax) / 2\n",
        "        y_center = (ymin + ymax) / 2\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        class_name = anno['name']\n",
        "        class_id = class_mapping[class_name] if class_name in class_mapping else -1\n",
        "\n",
        "        boxes.append([class_id, x_center, y_center, width, height])\n",
        "\n",
        "    return np.array(boxes)"
      ],
      "metadata": {
        "id": "3x4S_yFjvEDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    elif box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    assert type(bboxes) == list\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    boxes_after_nms = []\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            ) < iou_threshold\n",
        "        ]\n",
        "        boxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return boxes_after_nms"
      ],
      "metadata": {
        "id": "6nJiV4a-vFsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "architecture_config = [\n",
        "    (7, 64, 2, 3),  # Convolutional block 1\n",
        "    \"M\",            # Max-pooling layer 1\n",
        "    (3, 192, 1, 1), # Convolutional block 2\n",
        "    \"M\",            # Max-pooling layer 2\n",
        "    (1, 128, 1, 0), # Convolutional block 3\n",
        "    (3, 256, 1, 1), # Convolutional block 4\n",
        "    (1, 256, 1, 0), # Convolutional block 5\n",
        "    (3, 512, 1, 1), # Convolutional block 6\n",
        "    \"M\",            # Max-pooling layer 3\n",
        "    [(1, 256, 1, 0), (3, 512, 1, 1), 4], # Convolutional block 7 (repeated 4 times)\n",
        "    (1, 512, 1, 0), # Convolutional block 8\n",
        "    (3, 1024, 1, 1),# Convolutional block 9\n",
        "    \"M\",            # Max-pooling layer 4\n",
        "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2], # Convolutional block 10 (repeated 2 times)\n",
        "    (3, 1024, 1, 1),# Convolutional block 11\n",
        "    (3, 1024, 2, 1),# Convolutional block 12\n",
        "    (3, 1024, 1, 1),# Convolutional block 13\n",
        "    (3, 1024, 1, 1),# Convolutional block 14\n",
        "]\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "class Yolov1(nn.Module):\n",
        "    def __init__(self, in_channels=3, **kwargs):\n",
        "        super(Yolov1, self).__init__()\n",
        "        self.architecture = architecture_config\n",
        "        self.in_channels = in_channels\n",
        "        self.darknet = self._create_conv_layers(self.architecture)\n",
        "        self.fcs = self._create_fcs(**kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.darknet(x)\n",
        "        return self.fcs(torch.flatten(x, start_dim=1))\n",
        "\n",
        "    def _create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == tuple:\n",
        "                layers += [\n",
        "                    CNNBlock(\n",
        "                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n",
        "                    )\n",
        "                ]\n",
        "                in_channels = x[1]\n",
        "            elif type(x) == str:\n",
        "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
        "            elif type(x) == list:\n",
        "                conv1 = x[0]\n",
        "                conv2 = x[1]\n",
        "                num_repeats = x[2]\n",
        "\n",
        "                for _ in range(num_repeats):\n",
        "                    layers += [\n",
        "                        CNNBlock(\n",
        "                            in_channels,\n",
        "                            conv1[1],\n",
        "                            kernel_size=conv1[0],\n",
        "                            stride=conv1[2],\n",
        "                            padding=conv1[3],\n",
        "                        )\n",
        "                    ]\n",
        "                    layers += [\n",
        "                        CNNBlock(\n",
        "                            conv1[1],\n",
        "                            conv2[1],\n",
        "                            kernel_size=conv2[0],\n",
        "                            stride=conv2[2],\n",
        "                            padding=conv2[3],\n",
        "                        )\n",
        "                    ]\n",
        "                    in_channels = conv2[1]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
        "        S, B, C = split_size, num_boxes, num_classes\n",
        "        return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 4096),\n",
        "            nn.Dropout(0.0),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096, S * S * (C + B * 5)),\n",
        "        )"
      ],
      "metadata": {
        "id": "Fk4acu6CvJZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=20):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
        "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
        "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
        "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "        exists_box = target[..., 20].unsqueeze(3)\n",
        "\n",
        "        # Box coordinates loss\n",
        "        box_predictions = exists_box * (\n",
        "            bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25]\n",
        "        )\n",
        "        box_targets = exists_box * target[..., 21:25]\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "            torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "        )\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "        box_loss = self.mse(\n",
        "            torch.flatten(box_predictions, end_dim=-2),\n",
        "            torch.flatten(box_targets, end_dim=-2),\n",
        "        )\n",
        "\n",
        "        # Object loss\n",
        "        pred_box = (\n",
        "            bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
        "        )\n",
        "        object_loss = self.mse(\n",
        "            torch.flatten(exists_box * pred_box),\n",
        "            torch.flatten(exists_box * target[..., 20:21]),\n",
        "        )\n",
        "\n",
        "        # No object loss\n",
        "        no_object_loss = self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
        "        )\n",
        "        no_object_loss += self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
        "        )\n",
        "\n",
        "        # Class loss\n",
        "        class_loss = self.mse(\n",
        "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n",
        "            torch.flatten(exists_box * target[..., :20], end_dim=-2),\n",
        "        )\n",
        "\n",
        "        loss = (\n",
        "            self.lambda_coord * box_loss\n",
        "            + object_loss\n",
        "            + self.lambda_noobj * no_object_loss\n",
        "            + class_loss\n",
        "        )\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "bBTAykSrvK0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Hyperparameters and configurations\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 300\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "LOAD_MODEL_FILE = \"yolov1.pth.tar\"\n",
        "\n",
        "# Define data transformations\n",
        "WIDTH = 448\n",
        "HEIGHT = 448\n",
        "\n",
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.OneOf([\n",
        "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.9),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)\n",
        "        ], p=0.9),\n",
        "        A.ToGray(p=0.01),\n",
        "        A.HorizontalFlip(p=0.2),\n",
        "        A.VerticalFlip(p=0.2),\n",
        "        A.Resize(height=WIDTH, width=WIDTH, p=1),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']))\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(height=WIDTH, width=WIDTH, p=1.0),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], bbox_params=A.BboxParams(format='yolo', min_area=0, min_visibility=0, label_fields=['labels']))\n",
        "\n",
        "# Define class mapping\n",
        "class_mapping = {\n",
        "    \"aeroplane\": 0, \"bicycle\": 1, \"bird\": 2, \"boat\": 3, \"bottle\": 4,\n",
        "    \"bus\": 5, \"car\": 6, \"cat\": 7, \"chair\": 8, \"cow\": 9,\n",
        "    \"diningtable\": 10, \"dog\": 11, \"horse\": 12, \"motorbike\": 13, \"person\": 14,\n",
        "    \"pottedplant\": 15, \"sheep\": 16, \"sofa\": 17, \"train\": 18, \"tvmonitor\": 19\n",
        "}\n",
        "\n",
        "# Define training and evaluation functions\n",
        "def train_fn(train_loader, model, optimizer, loss_fn, epoch):\n",
        "    model.train()\n",
        "    mean_loss = []\n",
        "    mean_mAP = []\n",
        "\n",
        "    total_batches = len(train_loader)\n",
        "    display_interval = total_batches // 5\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred_boxes, true_boxes = get_boxes_training(out, y, iou_threshold=0.5, threshold=0.4)\n",
        "        mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
        "\n",
        "        mean_loss.append(loss.item())\n",
        "        mean_mAP.append(mAP.item())\n",
        "\n",
        "        if batch_idx % display_interval == 0 or batch_idx == total_batches - 1:\n",
        "            print(f\"Epoch: {epoch:3} \\t Iter: {batch_idx:3}/{total_batches:3} \\t Loss: {loss.item():3.10f} \\t mAP: {mAP.item():3.10f}\")\n",
        "\n",
        "    avg_loss = sum(mean_loss) / len(mean_loss)\n",
        "    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n",
        "    print(colored(f\"Train \\t loss: {avg_loss:3.10f} \\t mAP: {avg_mAP:3.10f}\", 'green'))\n",
        "    return avg_mAP\n",
        "\n",
        "def test_fn(test_loader, model, loss_fn, epoch):\n",
        "    model.eval()\n",
        "    mean_loss = []\n",
        "    mean_mAP = []\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(test_loader):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        pred_boxes, true_boxes = get_boxes_training(out, y, iou_threshold=0.5, threshold=0.4)\n",
        "        mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
        "\n",
        "        mean_loss.append(loss.item())\n",
        "        mean_mAP.append(mAP.item())\n",
        "\n",
        "    avg_loss = sum(mean_loss) / len(mean_loss)\n",
        "    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n",
        "    print(colored(f\"Test \\t loss: {avg_loss:3.10f} \\t mAP: {avg_mAP:3.10f}\", 'yellow'))\n",
        "    return avg_mAP\n",
        "\n",
        "# Main training function\n",
        "def train():\n",
        "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    loss_fn = YoloLoss()\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "    train_dataset = CustomVOCDataset(\n",
        "        root='./data',\n",
        "        year='2012',\n",
        "        image_set='train',\n",
        "        download=True,\n",
        "    )\n",
        "    train_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_train_transforms())\n",
        "\n",
        "    testval_dataset = CustomVOCDataset(\n",
        "        root='./data',\n",
        "        year='2012',\n",
        "        image_set='val',\n",
        "        download=True,\n",
        "    )\n",
        "    testval_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_valid_transforms())\n",
        "\n",
        "    dataset_size = len(testval_dataset)\n",
        "    val_size = int(0.15 * dataset_size)\n",
        "    test_size = dataset_size - val_size\n",
        "\n",
        "    val_indices = list(range(val_size))\n",
        "    test_indices = list(range(val_size, val_size + test_size))\n",
        "\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        dataset=testval_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        sampler=val_sampler,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        dataset=testval_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        sampler=test_sampler,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    best_mAP_train = 0\n",
        "    best_mAP_val = 0\n",
        "    best_mAP_test = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        train_mAP = train_fn(train_loader, model, optimizer, loss_fn, epoch)\n",
        "        val_mAP = test_fn(val_loader, model, loss_fn, epoch)\n",
        "        test_mAP = test_fn(test_loader, model, loss_fn, epoch)\n",
        "\n",
        "        if train_mAP > best_mAP_train:\n",
        "            best_mAP_train = train_mAP\n",
        "        if val_mAP > best_mAP_val:\n",
        "            best_mAP_val = val_mAP\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
        "        if test_mAP > best_mAP_test:\n",
        "            best_mAP_test = test_mAP\n",
        "\n",
        "        print(colored(f\"Best Train mAP: {best_mAP_train:3.10f}\", 'green'))\n",
        "        print(colored(f\"Best Val mAP: {best_mAP_val:3.10f}\", 'blue'))\n",
        "        print(colored(f\"Best Test mAP: {best_mAP_test:3.10f}\", 'yellow'))\n",
        "\n",
        "# Run training\n",
        "train()"
      ],
      "metadata": {
        "id": "YK_hrrvWvNIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image_with_labels(image, ground_truth_boxes, predicted_boxes, class_mapping):\n",
        "    inverted_class_mapping = {v: k for k, v in class_mapping.items()}\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(im)\n",
        "\n",
        "    for box in ground_truth_boxes:\n",
        "        label_index, box = box[0], box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"green\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n",
        "        ax.text(upper_left_x * width, upper_left_y * height, class_name, color='white', fontsize=12, bbox=dict(facecolor='green', alpha=0.2))\n",
        "\n",
        "    for box in predicted_boxes:\n",
        "        label_index, box = box[0], box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"r\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        class_name = inverted_class_mapping.get(label_index, \"Unknown\")\n",
        "        ax.text(upper_left_x * width, upper_left_y * height, class_name, color='white', fontsize=12, bbox=dict(facecolor='red', alpha=0.2))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def test():\n",
        "    model = Yolov1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        model.load_state_dict(torch.load(LOAD_MODEL_FILE)['state_dict'])\n",
        "\n",
        "    test_dataset = CustomVOCDataset(root='./data', image_set='val', download=False)\n",
        "    test_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_valid_transforms())\n",
        "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, shuffle=False, drop_last=False)\n",
        "\n",
        "    model.eval()\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(DEVICE)\n",
        "        out = model(x)\n",
        "\n",
        "        pred_bboxes = cellboxes_to_boxes(out)\n",
        "        gt_bboxes = cellboxes_to_boxes(y)\n",
        "\n",
        "        for idx in range(8):\n",
        "            pred_box = non_max_suppression(pred_bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
        "            gt_box = non_max_suppression(gt_bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
        "            image = x[idx].permute(1,2,0).to(\"cpu\")/255\n",
        "            plot_image_with_labels(image, gt_box, pred_box, class_mapping)\n",
        "        break\n",
        "\n",
        "# Run test\n",
        "test()"
      ],
      "metadata": {
        "id": "8zjY21JnvQKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}